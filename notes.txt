Since LEPL already works with general streams I think that adding a lexer can be
broken into two separate steps, both relatively simple.

First, provide ways of generating streams of streams.  For example, a list of
strings is a stream of streams.  But it would be more useful to have "real" streams,
since they give context info for error messages.  And that implies cleaning up the
stream interface (ABC?).

Second, provide a way to "drop down" into the embedded stream.  This means both
matching the token in some way and invoking a "sub-matcher" on the token's stream.

Obviously these need to cooperate on the exact format of the tokens.  The simplest
solution seems to be a (value, stream) pair, where the value is something that
indicates the type of token.

A compact syntax would be nice, since matching tokens is going to be something that
happens a lot.  The (value, stream) pair suggests an operator (which would go
between the two.  That's a bit of a pity, since I've pretty much exhausted the use
of operators.  On the other hand, it would be possible to "repurpose" some via a
with scope, and /, // would not be needed (I assume) with tokens.  So you could
imagine:

  with tokens():
    print  = ATOM // Literal('print')
    number = INT // Integer()

But even if that works with strings (forcing rdiv) it would fail with integer tags
(since // is already defined for that type).

Another approach might be to make something more general that somehow allows
matchers to be used on both halves of the pair in some way.  

MORE

Or there could be a new type, specially for matching tokens, that has its own
operators.  How would these differ from normal operators?  Again, / and // seem like
they could be reused, so you might have:

Tk(A) / Tk(B) // matchers...

to mean apply matchers to rhs of tokens A or B.  But that might just as easily (and
more compactly) be written

Tk(A, B) // matchers...

or even 

Tk(A, B, matchers...)

On a similar vein, perhaps you define Tokens elsewhere and then use the instances:

atom = Token('[a-z][a-z0-9]*')
number = Token(...)

assign = (atom > 'name') / '=' / (number > 'value')

parser = assign.string_parser(tokens=[atom, number],
                              config=...)

And then you could even auto-extract the tokens from the matchers?  Or use a "with"
scope to collect them?




----------------

On Apr 9, 2:56 am, David Liang <bmda...@gmail.com> wrote:
> Hi all,
> I'm having a weird problem with a regular expression (tested in 2.6
> and 3.0):
>
> Basically, any of these:
> _re_comments = re.compile(r'^(([^\\]+|\\.|"([^"\\]+|\\.)*")*)#.*$')
> _re_comments = re.compile(r'^(([^#]+|\\.|"([^"\\]+|\\.)*")*)#.*$')
> _re_comments = re.compile(r'^(([^"]+|\\.|"([^"\\]+|\\.)*")*)#.*$')
>
> followed by for example,
> line = r'~/.[m]ozilla/firefox/*.default/chrome'
> print(_re_comments.sub(r'\1', line))
>
> ...hangs the interpreter. For reference, if the first command had been
> _re_comments = re.compile(r'^(([^z]+|\\.|"([^"\\]+|\\.)*")*)#.*$')
>
> (off by one character z) it works fine, and all the equivalent
> operations work in sed and awk. Am I missing something about Python
> RE's?
>
> -David

The problem was the redundant +'s; the fixed RE is

    _re_comments = re.compile(r'^(([^#"\\]|\\.|"([^"\\]|\\.)*")*)#.*')


-------------------

Also, safely evaluating an expression.

